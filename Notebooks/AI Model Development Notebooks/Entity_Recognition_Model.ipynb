{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Entity Recognition Model Development\n",
    "\n",
    "This notebook focuses on developing a Named Entity Recognition (NER) model specifically for financial entities in both English and Swahili text. The model will be used to extract key financial information from user queries in the PesaGuru chatbot system.\n",
    "\n",
    "## Key Entities to Recognize:\n",
    "- MONEY_AMOUNT (e.g., \"KES 5000\", \"5k\", \"1M\")\n",
    "- TIME_PERIOD (e.g., \"2 years\", \"6 months\")\n",
    "- FINANCIAL_INSTRUMENT (e.g., \"stocks\", \"bonds\", \"T-bills\")\n",
    "- INSTITUTION (e.g., \"KCB\", \"Equity Bank\")\n",
    "- CURRENCY (e.g., \"KES\", \"USD\")\n",
    "- PERCENTAGE (e.g., \"5%\", \"10.5%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load financial corpus data\n",
    "with open('../data/external/financial_corpus.json', 'r', encoding='utf-8') as f:\n",
    "    financial_data = json.load(f)\n",
    "\n",
    "# Load Swahili corpus data for multilingual support\n",
    "with open('../data/external/swahili_corpus.json', 'r', encoding='utf-8') as f:\n",
    "    swahili_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_training_data(data):\n",
    "    \"\"\"Convert annotated data into spaCy's training format\"\"\"\n",
    "    training_data = []\n",
    "    for item in data:\n",
    "        text = item['text']\n",
    "        entities = item['entities']\n",
    "        training_data.append((text, {'entities': entities}))\n",
    "    return training_data\n",
    "\n",
    "# Prepare training data\n",
    "english_training_data = prepare_training_data(financial_data)\n",
    "swahili_training_data = prepare_training_data(swahili_data)\n",
    "\n",
    "# Combine datasets\n",
    "all_training_data = english_training_data + swahili_training_data\n",
    "random.shuffle(all_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_spacy_model():\n",
    "    \"\"\"Create a new spaCy model with custom NER pipeline\"\"\"\n",
    "    nlp = spacy.blank('en')  # we'll handle multilingual text in preprocessing\n",
    "    \n",
    "    # Create new NER pipeline\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe('ner', last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "    \n",
    "    # Add entity labels\n",
    "    for _, annotations in all_training_data:\n",
    "        for ent in annotations.get('entities', []):\n",
    "            ner.add_label(ent[2])\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(nlp, training_data, n_iter=100):\n",
    "    \"\"\"Train the NER model\"\"\"\n",
    "    # Get names of other pipes to disable during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        \n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(training_data)\n",
    "            losses = {}\n",
    "            \n",
    "            # Batch the examples and iterate over them\n",
    "            batches = minibatch(training_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                          losses=losses)\n",
    "            \n",
    "            print(f'Iteration {itn+1}, Losses:', losses)\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train the model\n",
    "nlp = create_spacy_model()\n",
    "trained_model = train_model(nlp, all_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    \"\"\"Evaluate model performance on test data\"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for text, annot in test_data:\n",
    "        doc = model(text)\n",
    "        gold_entities = set([(start, end, label) for start, end, label in annot['entities']])\n",
    "        pred_entities = set([(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n",
    "        \n",
    "        true_positives += len(gold_entities & pred_entities)\n",
    "        false_positives += len(pred_entities - gold_entities)\n",
    "        false_negatives += len(gold_entities - pred_entities)\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into train and test sets\n",
    "split = int(len(all_training_data) * 0.8)\n",
    "train_data = all_training_data[:split]\n",
    "test_data = all_training_data[split:]\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(trained_model, test_data)\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Testing and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def test_entity_recognition(model, text):\n",
    "    \"\"\"Test the model on a given text and display recognized entities\"\"\"\n",
    "    doc = model(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"I want to invest KES 50000 in government bonds for 2 years\",\n",
    "    \"What is the current interest rate at Equity Bank?\",\n",
    "    \"Nataka kuwekeza shilingi 10000 kwa hisa za Safaricom\",  # Swahili: I want to invest 10000 shillings in Safaricom shares\n",
    "    \"The stock gained 5.5% in value today\"\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    print(f\"\\nText: {text}\")\n",
    "    entities = test_entity_recognition(trained_model, text)\n",
    "    print(\"Recognized entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the trained model\n",
    "output_dir = Path('../server/ai/models/entity_recognition')\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True)\n",
    "\n",
    "trained_model.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Usage Instructions\n",
    "\n",
    "To use the trained model in the PesaGuru application:\n",
    "\n",
    "1. Load the model:\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load('../server/ai/models/entity_recognition')\n",
    "```\n",
    "\n",
    "2. Process text:\n",
    "```python\n",
    "text = \"I want to invest KES 50000 in government bonds\"\n",
    "doc = nlp(text)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "```\n",
    "\n",
    "The model will identify and classify financial entities in both English and Swahili text, making it suitable for the multilingual requirements of PesaGuru."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
